{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>ChromaDB Data Pipes is a collection of tools to build data pipelines for Chroma DB, inspired by the Unix philosophy of \" do one thing and do it well\".</p> <p>Roadmap:</p> <ul> <li>\u2705 Integration with LangChain \ud83e\udd9c\ud83d\udd17</li> <li>\ud83d\udeab Integration with LlamaIndex \ud83e\udd99</li> <li>\u2705 Support more than <code>all-MiniLM-L6-v2</code> as embedding functions (head over   to Embedding Processors for more info)</li> <li>\ud83d\udeab Multimodal support</li> <li>\u267e\ufe0f Much more!</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install chromadb-data-pipes\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>Get help:</p> <pre><code>cdp --help\n</code></pre>"},{"location":"#importing","title":"Importing","text":"<p>Import data from HuggingFace Datasets to <code>.jsonl</code> file:</p> <pre><code>cdp ds-get \"hf://tazarov/chroma-qna?split=train\" &gt; chroma-qna.jsonl\n</code></pre> <p>Import data from HuggingFace Datasets to Chroma DB Server:</p> <p>The below command will import the <code>train</code> split of the given dataset to Chroma chroma-qna <code>chroma-qna</code> collection. The collection will be created if it does not exist and documents will be upserted.</p> <pre><code>cdp ds-get \"hf://tazarov/chroma-qna?split=train\" | cdp import \"http://localhost:8000/chroma-qna\" --upsert --create\n</code></pre> <p>Importing from a directory with PDF files into Local Persisted Chroma DB:</p> <pre><code>cdp ds-get sample-data/papers/ | grep \"2401.02412.pdf\" | head -1 | cdp chunk -s 500 | cdp embed --ef default | cdp import \"file://chroma-data/my-pdfs\" --upsert --create\n</code></pre> <p>Note</p> <p>The above command will import the first PDF file from the <code>sample-data/papers/</code> directory, chunk it into 500 word chunks, embed each chunk and import the chunks to the <code>my-pdfs</code> collection in Chroma DB.</p>"},{"location":"#exporting","title":"Exporting","text":"<p>Export data from Local Persisted Chroma DB to <code>.jsonl</code> file:</p> <p>The below command will export the first 10 documents from the <code>chroma-qna</code> collection to <code>chroma-qna.jsonl</code> file.</p> <pre><code>cdp export \"file://chroma-data/chroma-qna\" --limit 10 &gt; chroma-qna.jsonl\n</code></pre> <p>Export data from Local Persisted Chroma DB to <code>.jsonl</code> file with filter:</p> <p>The below command will export data from local persisted Chroma DB to a <code>.jsonl</code> file using a <code>where</code> filter to select the documents to export.</p> <pre><code>cdp export \"file://chroma-data/chroma-qna\" --where '{\"document_id\": \"123\"}' &gt; chroma-qna.jsonl\n</code></pre> <p>Export data from Chroma DB Server to HuggingFace Datasets:</p> <p>The below command will export the first 10 documents with offset 10 from the <code>chroma-qna</code> collection to HuggingFace Datasets <code>tazarov/chroma-qna</code> dataset. The dataset will be uploaded to HF.</p> <p>Note</p> <p>Make sure you have <code>HF_TOKEN=hf_....</code> environment variable set. If you want your dataset to be private, add <code>--private</code> flag to the <code>cdp ds-put</code> command.</p> <pre><code>cdp export \"http://localhost:8000/chroma-qna\" --limit 10 --offset 10 | cdp ds-put \"hf://tazarov/chroma-qna-modified\"\n</code></pre> <p>To export a dataset to a file, use <code>--uri</code> with <code>file://</code> prefix:</p> <pre><code>cdp export \"http://localhost:8000/chroma-qna\" --limit 10 --offset 10 | cdp ds-put \"file://chroma-qna\"\n</code></pre> <p>Note</p> <p>The file is  relative to the current working directory.</p>"},{"location":"#processing","title":"Processing","text":"<p>Copy collection from one Chroma collection to another and re-embed the documents:</p> <pre><code>cdp export \"http://localhost:8000/chroma-qna\" | cdp embed --ef default | cdp import \"http://localhost:8000/chroma-qna-def-emb\" --upsert --create\n</code></pre> <p>Embeddings Processor</p> <p>See Embedding Processors for more info about supported embedding functions.</p> <p>Import dataset from HF to Local Persisted Chroma and embed the documents:</p> <pre><code>cdp ds-get \"hf://tazarov/ds2?split=train\" | cdp embed --ef default | cdp import \"file://chroma-data/chroma-qna-def-emb-hf\" --upsert --create\n</code></pre> <p>Chunk Large Documents:</p> <pre><code>cdp imp pdf sample-data/papers/ | grep \"2401.02412.pdf\" | head -1 | cdp chunk -s 500\n</code></pre>"},{"location":"#misc","title":"Misc","text":"<p>Count the number of documents in a collection:</p> <pre><code>cdp export \"http://localhost:8000/chroma-qna\" | wc -l\n</code></pre>"},{"location":"concepts/","title":"Concepts","text":""},{"location":"concepts/#embeddableresource","title":"EmbeddableResource","text":"<p>ChromaDB Data Pipes operates over a structure called EmbeddableResource. The structure is a closely related to ChromaDB Documents. All core components of the library will either, produce, consume, or transform EmbeddableResources.</p> <p>Each embeddable resource has the following properties:</p> <ul> <li><code>id</code> - unique identifier of the resource</li> <li><code>metadata</code> - metadata of the resource</li> <li><code>embedding</code> - embedding of the resource</li> </ul> <p>For text resource we use <code>EmbeddableTextResource</code> which adds the following properties:</p> <ul> <li><code>text_chunk</code> - text of the resource</li> </ul> <p>Evolution</p> <p>We plan to evolve the EmbeddableResource structure to support more types of resources, such as images, audio, video,</p>"},{"location":"concepts/#producer","title":"Producer","text":"<p>Generates a stream of data to a file or stdout.</p> <p>The source of the data is implementation dependent, HF datasets, ChromaDB, file etc.</p>"},{"location":"concepts/#consumer","title":"Consumer","text":"<p>Consumes a stream of data from a file or stdin.</p>"},{"location":"concepts/#processor","title":"Processor","text":"<p>Consumes a stream of data from a file or stdin and processes it by some criteria. Produces a stream of data to a file or stdout.</p>"},{"location":"concepts/#pipeline","title":"Pipeline","text":"<p>Reusable set of producer, consumer, filter, and transformer.</p> <p>WIP</p> <p>This is a work in progress. Stay tuned for more updates.</p>"},{"location":"adr/ADR-20240120-CLI-Naming/","title":"CLI Naming Conventions","text":"<p>Date: 2024-01-20 Tags: cli, naming, conventions Authors: @tazarov</p>"},{"location":"adr/ADR-20240120-CLI-Naming/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We want to be mindful of the naming conventions we use for the CLI. We want to be consistent and clear about the commands we use and the order of the arguments.</p>"},{"location":"adr/ADR-20240120-CLI-Naming/#considered-options","title":"Considered Options","text":"<ul> <li>Extensive use of subcommands</li> <li>Ecosystem centric naming</li> </ul>"},{"location":"adr/ADR-20240120-CLI-Naming/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Ecosystem centric naming\", because it will make the CLI more aligned with the ecosystem thus reducing the cognitive load on users.</p>"},{"location":"adr/ADR-20240120-CLI-Naming/#the-use-of-import-and-export-for-chroma","title":"The use of <code>import</code> and <code>export</code> for Chroma","text":"<p>This is a toolchain in the chroma ecosystem and we want to be very explicit about the use of import and export. They must convey the core idea behind CDP, getting data in and out of Chroma.</p>"},{"location":"adr/ADR-20240120-CLI-Naming/#the-use-of-embed","title":"The use of <code>embed</code>","text":"<p>A core concept around vector databases is embedding. We want to make the embedding a first class citizen of CDP this is why we choose to have embed as a top level command.</p>"},{"location":"adr/ADR-20240120-CLI-Naming/#the-use-of-chunk","title":"The use of <code>chunk</code>","text":"<p>Another core concept around vector databases and embedding models is context window. Each embedding model has a specific context window and sometimes the embedding models, make a trade-off to truncate data for better usability rather than warning or preventing the user from using the model with an oversized context window. To that end chunking (of mostly text data) has become a core concept in the ecosystem. We want to convey that by elevating the chunk command to a top level command.</p>"},{"location":"adr/ADR-20240120-CLI-Naming/#the-use-of-ds-get-and-ds-put","title":"The use of <code>ds-get</code> and <code>ds-put</code>","text":"<p>Working with datasets is another core concept in the econsystem. While we currently only plan to support HuggingFace datasets, we want to also put the datasets front and center in CDP CLI. This is why we choose to have <code>ds-get</code> and <code>ds-put</code> as top level commands.</p>"},{"location":"adr/ADR-20240120-CLI-Naming/#pros-and-cons-of-the-options","title":"Pros and Cons of the Options","text":""},{"location":"adr/ADR-20240120-CLI-Naming/#extensive-use-of-subcommands","title":"Extensive use of subcommands","text":"<ul> <li>Good, because it allows for a more natural grouping of commands</li> <li>Good, because it will solve a top-level command sprawl</li> <li>Bad, because it will put a burden on the user to remember and navigate the subcommands</li> </ul>"},{"location":"adr/ADR-20240120-CLI-Naming/#ecosystem-centric-naming","title":"Ecosystem centric naming","text":"<ul> <li>Good, because it will make the CLI more consistent with the ecosystem</li> <li>Good, because it reduces the cognitive load on users by aligning with expectations set by the ecosystem</li> <li>Bad, because it will increase the number of top-level commands</li> </ul>"},{"location":"processors/chunking/","title":"Chunking","text":""},{"location":"processors/chunking/#usage","title":"Usage","text":"<pre><code>cdp imp url &lt;url&gt; [flags]\n</code></pre> <p>The following example will chunk the document into 500 character chunks and print the chunks to stdout. We will also add (<code>-a</code> option) the offset position of each chunk within the document as metadata <code>start_index</code>.</p> <pre><code>cdp imp pdf sample-data/papers/ | grep \"2401.02412.pdf\" | head -1 | cdp chunk -s 500 -a\n</code></pre> <p>Alternatively you can chunk from an input <code>jsonl</code> file:</p> <pre><code>cdp imp pdf sample-data/papers/ | grep \"2401.02412.pdf\" | head -1 | cdp chunk -s 500 &gt; chunk.jsonl\n</code></pre> <p>jsonl format</p> <p>It is expected that the <code>jsonl</code> file contains <code>chroma_dp.EmbeddableTextResource</code> objects (one per line).</p> <pre><code>cdp chunk -s 500 --in chunk.jsonl\n</code></pre> <p>Help</p> <p>Run <code>cdp chunk --help</code> for more information.</p>"},{"location":"processors/cleaning/","title":"Cleaning Processors","text":""},{"location":"processors/cleaning/#clean-emoji","title":"Clean Emoji","text":"<p>Usage:</p> <pre><code>cdp tx emoji-clean [-m] [-|&lt;file&gt;]\n</code></pre> <p>Get Help</p> <p>Get help for the command with the following flag:</p> <pre><code>cdp tx emoji-clean --help\n</code></pre>"},{"location":"processors/cleaning/#example","title":"Example","text":"<p>The following example cleans the ChromaDB docs home page from emojis, including metadata.</p> <pre><code>cdp imp url https://docs.trychroma.com/ -d 1 | cdp tx emoji-clean -m\n</code></pre>"},{"location":"processors/embedding/","title":"Embedding Processors","text":""},{"location":"processors/embedding/#default-embedding-processor","title":"Default Embedding Processor","text":"<p>CDP comes with a default embedding processor that supports the following embedding functions:</p> <ul> <li>Default (<code>default</code>) - The default ChromaDB embedding function based on OnnxRuntime and MiniLM-L6-v2 model.</li> <li>OpenAI (<code>openai</code>) - OpenAI's text-embedding-ada-002 model.</li> <li>Cohere (<code>cohere</code>) - Cohere's embedding models.</li> <li>HuggingFrace (<code>hf</code>) - HuggingFace's embedding models.</li> <li>SentenceTransformers (<code>st</code>) - SentenceTransformers' embedding models.</li> </ul> <p>The embedding functions are based on ChromaDB's embedding functions.</p>"},{"location":"processors/embedding/#usage","title":"Usage","text":""},{"location":"processors/embedding/#default","title":"Default","text":"<p>The below command will read a PDF files at the specified path, filter the output for a particular pdf (<code>grep</code>). Select the first document's page, chunk it to 500 characters, embed each chunk using Chroma's default (MiniLM-L2-v2) model. The resulting documents with embeddings will be written to <code>chroma-data.jsonl</code> file.</p> <pre><code>cdp imp pdf sample-data/papers/ | cdp chunk -s 500 | cdp embed --ef default &gt; chroma-data.jsonl\n</code></pre>"},{"location":"processors/embedding/#openai","title":"OpenAI","text":"<p>To use this embedding function, you need to install the <code>openai</code> python package.</p> <pre><code>pip install openai\n</code></pre> <p>OpenAI API Key</p> <p>You need to have an OpenAI API key to use this embedding function. You can get an API key by signing up for an account at OpenAI. The API key must be exported as env variable <code>OPENAI_API_KEY=sk-xxxxxx</code>.</p> <p>The below command will read a PDF files at the specified path, filter the output for a particular pdf (<code>grep</code>). Select the first document's page, chunk it to 500 characters, embed each chunk using OpenAI's text-embedding-ada-002 model.</p> <pre><code>export OPENAI_API_KEY=sk-xxxxxx\ncdp imp pdf sample-data/papers/ |grep \"2401.02412.pdf\" | head -1 | cdp chunk -s 500 | cdp embed --ef openai\n</code></pre>"},{"location":"processors/embedding/#cohere","title":"Cohere","text":"<p>To use this embedding function, you need to install the <code>cohere</code> python package.</p> <pre><code>pip install cohere\n</code></pre> <p>Cohere API Key</p> <p>You need to have a Cohere API key to use this embedding function. You can get an API key by signing up for an account at Cohere. The API key must be exported as env variable <code>COHERE_API_KEY=x4q...</code>.</p> <p>Cohere Embedding Models</p> <p>By default, if not specified, the <code>embed-english-v3.0</code> model is used. You can pass in an optional <code>--model=embed-english-light-v3.0</code> argument or env variable <code>COHERE_MODEL_NAME=embed-multilingual-v3.0</code> , which lets you choose which Cohere embeddings model to use. More about available models can be found at Cohere's API docs</p> <p>The below command will read a PDF files at the specified path, select the last document's page, chunk it to 100 characters, embed each chunk using Cohere's embed-english-light-v3.0 model.</p> <pre><code>export COHERE_API_KEY=x4q\nexport COHERE_MODEL_NAME=\"embed-english-light-v3.0\"\ncdp imp pdf sample-data/papers/ | tail -1 | cdp chunk -s 100 | cdp embed --ef cohere\n</code></pre>"},{"location":"processors/embedding/#huggingface","title":"HuggingFace","text":"<p>HF API Token</p> <p>You need to have a HuggungFace API token to use this embedding function. Create or use one from your tokens page. The API key must be exported as env variable <code>HF_TOKEN=hf_xxxx</code>.</p> <p>HF Embedding Models</p> <p>By default, if not specified, the <code>sentence-transformers/all-MiniLM-L6-v2</code> model is used. You can pass in an optional <code>--model=BAAI/bge-large-en-v1.5</code> argument or env variable <code>HF_MODEL_NAME=BAAI/bge-large-en-v1.5</code> , which lets you choose which Cohere embeddings model to use.</p> <p>The below command will read a PDF files at the specified path, select the first two pages, chunk it to 150 characters, selects the last chunk and embeds the chunk using BAAI/bge-large-en-v1.5 model.</p> <pre><code>export HF_TOKEN=hf_xxxx\nexport HF_MODEL_NAME=\"BAAI/bge-large-en-v1.5\"\ncdp imp pdf sample-data/papers/ | head -2 | cdp chunk -s 150 | tail -1 | cdp embed --ef hf\n</code></pre>"},{"location":"processors/embedding/#sentencetransformers","title":"SentenceTransformers","text":"<p>To use this embedding function, you need to install the <code>sentence-transformers</code> python package.</p> <pre><code>pip install sentence-transformers\n</code></pre> <p>SentenceTransformers Embedding Models</p> <p>By default, if not specified, the <code>all-MiniLM-L6-v2</code> model is used. You can pass in an optional <code>--model=BAAI/bge-large-en-v1.5</code> argument or env variable <code>ST_MODEL_NAME=BAAI/bge-large-en-v1.5</code> , which lets you choose which Sentence Transformers embeddings model to use.</p> <p>The below command will read a PDF files at the specified path, select the first two pages, chunk it to 150 characters, selects the last chunk and embeds the chunk using BAAI/bge-small-en-v1.5 model.</p> <pre><code>export ST_MODEL_NAME=\"BAAI/bge-small-en-v1.5\"\ncdp imp pdf sample-data/papers/ | head -2 | cdp chunk -s 150 | tail -1 | cdp embed --ef st\n</code></pre>"},{"location":"producers/pdf/","title":"PDFs","text":""},{"location":"producers/pdf/#pypdf","title":"PyPDF","text":"<p>Reading a dir with PDF files to stdout:</p> <pre><code>cdp imp pdf sample-data/papers/\n</code></pre> <p>Help</p> <p>Run <code>cdp imp pdf --help</code> for more information.</p>"},{"location":"producers/text/","title":"Text Files","text":""},{"location":"producers/text/#default-text-file-generator","title":"Default Text File Generator","text":"<p>Reading a dir with text files to stdout:</p> <pre><code>cdp imp txt sample-data/text/ | head -1 | cdp chunk -s 100 | tail -1 | cdp embed --ef default\n</code></pre> <p>Help</p> <p>Run <code>cdp imp txt --help</code> for more information.</p>"},{"location":"producers/url/","title":"URL Importer","text":"<p>Imports data from a URL.</p>"},{"location":"producers/url/#default","title":"Default","text":""},{"location":"producers/url/#usage","title":"Usage","text":"<pre><code>cdp imp url &lt;url&gt; [flags]\n</code></pre> <p>Get Help</p> <p>Get help for the command with the following flag:</p> <pre><code>cdp imp url --help\n</code></pre> <p>URLs with binary data</p> <p>Currently the URL importer does not support URLs with binary data, such as PDFs or other binary files. Need this? Raise an issue here.</p> <p>The following example imports data from ChromaDB documentation with max depth 2.</p> <pre><code>cdp imp url https://docs.trychroma.com/embeddings -d 2\n</code></pre>"},{"location":"producers/url/#advanced-usage","title":"Advanced Usage","text":"<p>The following example imports data from ChromaDB documentation with max depth 3, chunks the data into 512 byte chunks, cleans the data of emojis, and embeds the data using the default embedding function.</p> <pre><code>cdp imp url https://docs.trychroma.com/ -d 3 | cdp chunk -s 512| cdp tx emoji-clean -m | cdp embed --ef default\n</code></pre>"}]}