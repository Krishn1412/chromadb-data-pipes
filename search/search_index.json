{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>ChromaDB Data Pipes is a collection of tools to build data pipelines for Chroma DB, inspired by the Unix philosophy of \" do one thing and do it well\".</p> <p>Roadmap:</p> <ul> <li>\u2705 Integration with LangChain \ud83e\udd9c\ud83d\udd17</li> <li>\ud83d\udeab Integration with LlamaIndex \ud83e\udd99</li> <li>\ud83d\udeab Support more than <code>all-MiniLM-L6-v2</code> as embedding functions</li> <li>\ud83d\udeab Multimodal support</li> <li>\u267e\ufe0f Much more!</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install chromadb-data-pipes\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>Get help:</p> <pre><code>cdp --help\n</code></pre>"},{"location":"#importing","title":"Importing","text":"<p>Import data from HuggingFace Datasets to <code>.jsonl</code> file:</p> <pre><code>cdp imp hf --uri \"hf://tazarov/chroma-qna?split=train\" &gt; chroma-qna.jsonl\n</code></pre> <p>Import data from HuggingFace Datasets to Chroma DB Server:</p> <p>The below command will import the <code>train</code> split of the given dataset to Chroma chroma-qna <code>chroma-qna</code> collection. The collection will be created if it does not exist and documents will be upserted.</p> <pre><code>cdp imp hf --uri \"hf://tazarov/chroma-qna?split=train\" | cdp imp chroma --uri \"http://localhost:8000/chroma-qna\" --upsert --create\n</code></pre> <p>Importing from a directory with PDF files into Local Persisted Chroma DB:</p> <pre><code>cdp imp pdf sample-data/papers/ | grep \"2401.02412.pdf\" | head -1 | cdp tx chunk -s 500 | cdp tx embed --ef default | cdp imp chroma --uri \"http://chroma-data/my-pdfs\" --upsert --create\n</code></pre> <p>Note</p> <p>The above command will import the first PDF file from the <code>sample-data/papers/</code> directory, chunk it into 500 word chunks, embed each chunk and import the chunks to the <code>my-pdfs</code> collection in Chroma DB.</p>"},{"location":"#exporting","title":"Exporting","text":"<p>Export data from Local Persisted Chroma DB to <code>.jsonl</code> file:</p> <p>The below command will export the first 10 documents from the <code>chroma-qna</code> collection to <code>chroma-qna.jsonl</code> file.</p> <pre><code>cdp exp chroma --uri \"http://chroma-data/chroma-qna\" --limit 10 &gt; chroma-qna.jsonl\n</code></pre> <p>Export data from Chroma DB Server to HuggingFace Datasets:</p> <p>The below command will export the first 10 documents with offset 10 from the <code>chroma-qna</code> collection to HuggingFace Datasets <code>tazarov/chroma-qna</code> dataset. The dataset will be uploaded to HF.</p> <p>Note</p> <p>Make sure you have <code>HF_TOKEN=hf_....</code> environment variable set. If you want your dataset to be private, add <code>--private</code> flag to the <code>cdp exp hf</code> command.</p> <pre><code>cdp exp chroma --uri \"http://localhost:8000/chroma-qna\" --limit 10 --offset 10 | cdp exp hf --uri \"hf://tazarov/chroma-qna-modified\"\n</code></pre> <p>To export a dataset to a file, use <code>--uri</code> with <code>file://</code> prefix:</p> <pre><code>cdp exp chroma --uri \"http://localhost:8000/chroma-qna\" --limit 10 --offset 10 | cdp exp hf --uri \"file://chroma-qna\"\n</code></pre> <p>Note</p> <p>The file is  relative to the current working directory.</p>"},{"location":"#processing","title":"Processing","text":"<p>Copy collection from one Chroma collection to another and re-embed the documents:</p> <pre><code>cdp exp chroma --uri \"http://localhost:8000/chroma-qna\" | cdp tx embed --ef default | cdp imp chroma --uri \"http://localhost:8000/chroma-qna-def-emb\" --upsert --create\n</code></pre> <p>Embeddings Processor</p> <p>See Embedding Processors for more info about supported embedding functions.</p> <p>Import dataset from HF to Local Persisted Chroma and embed the documents:</p> <pre><code>cdp imp hf --uri \"hf://tazarov/ds2?split=train\" | cdp tx embed --ef default | cdp imp chroma --uri \"file://chroma-data/chroma-qna-def-emb-hf\" --upsert --create\n</code></pre> <p>Chunk Large Documents:</p> <pre><code>cdp imp pdf sample-data/papers/ | grep \"2401.02412.pdf\" | head -1 | cdp tx chunk -s 500\n</code></pre>"},{"location":"#misc","title":"Misc","text":"<p>Count the number of documents in a collection:</p> <pre><code>cdp exp chroma --uri \"http://localhost:8000/chroma-qna\" | wc -l\n</code></pre>"},{"location":"concepts/","title":"CDP Concepts","text":""},{"location":"concepts/#producer","title":"Producer","text":"<p>Generates a stream of data to a file or stdout.</p> <p>The source of the data is implementation dependent, HF datasets, ChromaDB, file etc.</p>"},{"location":"concepts/#consumer","title":"Consumer","text":"<p>Consumes a stream of data from a file or stdin.</p>"},{"location":"concepts/#processor","title":"Processor","text":"<p>Consumes a stream of data from a file or stdin and processes it by some criteria. Produces a stream of data to a file or stdout.</p>"},{"location":"concepts/#pipeline","title":"Pipeline","text":"<p>Reusable set of producer, consumer, filter, and transformer.</p> <p>Properties: - Variables</p>"},{"location":"processors/chunking/","title":"Chunking","text":"<p>To chunk documents we have a chunk processor - <code>cdp tx chunk</code> that can be used as follows:</p> <pre><code>cdp imp pdf sample-data/papers/ | grep \"2401.02412.pdf\" | head -1 | cdp tx chunk -s 500 -a\n</code></pre> <p>The above will chunk the document into 500 character chunks and print the chunks to stdout. We will also add (<code>-a</code> option) the offset position of each chunk within the document as metadata <code>start_index</code>.</p> <p>Alternatively you can chunk from an input <code>jsonl</code> file:</p> <p>Create a <code>jsonl</code> file</p> <pre><code>cdp imp pdf sample-data/papers/ | grep \"2401.02412.pdf\" | head -1 | &gt; chunk.jsonl\n</code></pre> <p>jsonl format</p> <p>It is expected that the <code>jsonl</code> file contains <code>chroma_dp.EmbeddableTextResource</code> objects (one per line).</p> <pre><code>cdp tx chunk -s 500 --in chunk.jsonl\n</code></pre> <p>Help</p> <p>Run <code>cdp tx chunk --help</code> for more information.</p>"},{"location":"processors/embedding/","title":"Embedding Processors","text":""},{"location":"processors/embedding/#default","title":"Default","text":"<p>CDP comes with a default embedding processor that supports the following embedding functions:</p> <ul> <li>Default (<code>default</code>) - The default ChromaDB embedding function based on OnnxRuntime and MiniLM-L6-v2 model.</li> <li>OpenAI (<code>openai</code>) - OpenAI's text-embedding-ada-002 model.</li> <li>Cohere (<code>cohere</code>) - Cohere's embedding models.</li> </ul>"},{"location":"processors/embedding/#usage","title":"Usage","text":""},{"location":"processors/embedding/#default_1","title":"Default","text":"<p>The below command will read a PDF files at the specified path, filter the output for a particular pdf (<code>grep</code>). Select the first document's page, chunk it to 500 characters, embed each chunk using Chroma's default (MiniLM-L2-v2) model. The resulting documents with embeddings will be written to <code>chroma-data.jsonl</code> file.</p> <pre><code>cdp imp pdf sample-data/papers/ | cdp tx chunk -s 500 | cdp tx embed --ef default &gt; chroma-data.jsonl\n</code></pre>"},{"location":"processors/embedding/#openai","title":"OpenAI","text":"<p>OpenAI API Key</p> <p>You need to have an OpenAI API key to use this embedding function. You can get an API key by signing up for an account at OpenAI. The API key must be exported as env variable <code>OPENAI_API_KEY=sk-xxxxxx</code>.</p> <p>The below command will read a PDF files at the specified path, filter the output for a particular pdf (<code>grep</code>). Select the first document's page, chunk it to 500 characters, embed each chunk using OpenAI's text-embedding-ada-002 model.</p> <pre><code>cdp imp pdf sample-data/papers/ |grep \"2401.02412.pdf\" | head -1 | cdp tx chunk -s 500 | cdp tx embed --ef openai\n</code></pre>"},{"location":"processors/embedding/#cohere","title":"Cohere","text":"<p>Cohere API Key</p> <p>You need to have a Cohere API key to use this embedding function. You can get an API key by signing up for an account at Cohere. The API key must be exported as env variable <code>COHERE_API_KEY=x4q...</code>.</p> <p>Cohere Embedding Models</p> <p>By default, if not specified, the <code>embed-english-v3.0</code> model is used. You can pass in an optional <code>--model=embed-english-light-v3.0</code> argument or env variable <code>COHERE_MODEL_NAME=embed-multilingual-v3.0</code> , which lets you choose which Cohere embeddings model to use. More about available models can be found at Cohere's API docs</p> <p>The below command will read a PDF files at the specified path, select the last document's page, chunk it to 100 characters, embed each chunk using Cohere's embed-english-light-v3.0 model.</p> <pre><code>export COHERE_API_KEY=x4q\nexport COHERE_MODEL_NAME=embed-english-light-v3.0\ncdp imp pdf sample-data/papers/ | tail -1 | cdp tx chunk -s 100 | cdp tx embed --ef cohere\n</code></pre>"},{"location":"producers/pdf/","title":"PDFs","text":""},{"location":"producers/pdf/#pypdf","title":"PyPDF","text":"<p>Reading a dir with PDF files to stdout:</p> <pre><code>cdp imp pdf sample-data/papers/\n</code></pre> <p>Help</p> <p>Run <code>cdp imp pdf --help</code> for more information.</p>"}]}